{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Review Detection: End-to-End Modeling Pipeline\n",
    "\n",
    "This notebook implements a complete end-to-end pipeline for the fake review detection model:\n",
    "1. Data Collection\n",
    "2. Preprocessing\n",
    "3. Feature Engineering\n",
    "4. Model Training\n",
    "5. Evaluation\n",
    "6. Model Interpretation\n",
    "7. Model Serialization\n",
    "\n",
    "The final model will be saved to `artifacts/models/production_model.joblib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import project modules\n",
    "from src.data_collection import DataCollector\n",
    "from src.preprocessing import TextPreprocessor\n",
    "from src.feature_engineering import FeatureEngineer\n",
    "from src.modeling import ModelTrainer\n",
    "from src.evaluation import Evaluation\n",
    "from src.interpretation import ModelInterpreter\n",
    "from src.utils import setup_logging, ensure_dir\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Configure warnings and setup logger\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = setup_logging('modeling_notebook')\n",
    "\n",
    "# Configure paths\n",
    "DATA_DIR = Path('../data')\n",
    "RAW_DATA_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "ARTIFACTS_DIR = Path('../artifacts')\n",
    "MODELS_DIR = ARTIFACTS_DIR / 'models'\n",
    "REPORTS_DIR = ARTIFACTS_DIR / 'reports'\n",
    "\n",
    "# Ensure directories exist\n",
    "for dir_path in [PROCESSED_DATA_DIR, MODELS_DIR, REPORTS_DIR]:\n",
    "    ensure_dir(dir_path)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection\n",
    "\n",
    "This step collects data from available sources using the `DataCollector` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data collector\n",
    "collector = DataCollector()\n",
    "\n",
    "# Path to your existing datasets\n",
    "# Update these paths based on your environment\n",
    "yelp_dataset_path = RAW_DATA_DIR / 'yelp'\n",
    "amazon_dataset_path = RAW_DATA_DIR / 'amazon'\n",
    "\n",
    "# Check available data\n",
    "available_yelp_files = list(yelp_dataset_path.glob('*.parquet'))\n",
    "available_amazon_files = list(amazon_dataset_path.glob('*.parquet'))\n",
    "\n",
    "print(f\"Available Yelp files: {available_yelp_files}\")\n",
    "print(f\"Available Amazon files: {available_amazon_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine available datasets\n",
    "# We'll use a simulated approach since we don't know the exact file structure\n",
    "\n",
    "# Helper function to load data\n",
    "def load_data_from_paths(file_paths, source_label=None):\n",
    "    dfs = []\n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            df = pd.read_parquet(path)\n",
    "            if source_label:\n",
    "                df['source'] = source_label\n",
    "            dfs.append(df)\n",
    "            print(f\"Loaded {len(df)} records from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "    \n",
    "    if dfs:\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    else:\n",
    "        # If no data found, create a small sample dataset for demonstration\n",
    "        print(\"No data found, creating sample dataset for demonstration\")\n",
    "        return create_sample_dataset(source_label)\n",
    "\n",
    "# Function to create a sample dataset if no data is available\n",
    "def create_sample_dataset(source_label):\n",
    "    # Create synthetic data for demonstration purposes\n",
    "    n_samples = 1000\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    # Generate some fake review texts of varying length\n",
    "    texts = [\n",
    "        \"This product is amazing! I love it so much.\",\n",
    "        \"Terrible quality. Would not recommend to anyone.\",\n",
    "        \"Good value for money. Works as expected.\",\n",
    "        \"The service was excellent and the staff very friendly.\",\n",
    "        \"Not worth the price. Broke after just two weeks of use.\",\n",
    "        \"I'm extremely satisfied with my purchase! Will buy again.\",\n",
    "        \"Average product, nothing special but gets the job done.\",\n",
    "        \"Amazing product! I received so many compliments! Best purchase ever!!! HIGHLY RECOMMEND!!\",\n",
    "        \"This was just okay. Not great, not terrible.\",\n",
    "        \"Worst experience ever. Don't waste your money.\"\n",
    "    ]\n",
    "    \n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'review_id': [f\"review_{i}\" for i in range(n_samples)],\n",
    "        'user_id': [f\"user_{np.random.randint(1, 100)}\" for _ in range(n_samples)],\n",
    "        'product_id': [f\"product_{np.random.randint(1, 50)}\" for _ in range(n_samples)],\n",
    "        'review_text': [np.random.choice(texts) for _ in range(n_samples)],\n",
    "        'rating': np.random.randint(1, 6, size=n_samples),\n",
    "        'date': pd.date_range(start='2020-01-01', periods=n_samples),\n",
    "        'source': source_label,\n",
    "        # Target variable - 1 for fake review, 0 for genuine\n",
    "        'is_fake': np.random.binomial(1, 0.3, size=n_samples)\n",
    "    })\n",
    "    \n",
    "    # Add some synthetic features that might correlate with fake reviews\n",
    "    # Fake reviews might have more extreme ratings\n",
    "    df.loc[df['is_fake'] == 1, 'rating'] = df.loc[df['is_fake'] == 1, 'rating'].apply(\n",
    "        lambda x: np.random.choice([1, 5], p=[0.3, 0.7])\n",
    "    )\n",
    "    \n",
    "    # Fake reviews might be from users with fewer reviews\n",
    "    df['user_review_count'] = df.groupby('user_id')['review_id'].transform('count')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "yelp_data = load_data_from_paths(available_yelp_files, 'yelp')\n",
    "amazon_data = load_data_from_paths(available_amazon_files, 'amazon')\n",
    "\n",
    "# Combine datasets\n",
    "combined_data = pd.concat([yelp_data, amazon_data], ignore_index=True)\n",
    "print(f\"Combined dataset shape: {combined_data.shape}\")\n",
    "\n",
    "# Display sample\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "This step cleans and preprocesses the text data using the `TextPreprocessor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text preprocessor\n",
    "preprocessor = TextPreprocessor(\n",
    "    text_column='review_text',\n",
    "    target_column='is_fake',\n",
    "    user_id_column='user_id',\n",
    "    product_id_column='product_id',\n",
    "    date_column='date',\n",
    "    rating_column='rating',\n",
    "    lowercase=True,\n",
    "    remove_html=True,\n",
    "    remove_urls=True,\n",
    "    remove_emails=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=True,\n",
    "    stem=False,\n",
    "    handle_missing='fill',\n",
    "    temporal_check=True,\n",
    "    feature_engineering=True,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_data = preprocessor.fit_transform(combined_data)\n",
    "\n",
    "# Check the processed data\n",
    "print(f\"Processed data shape: {processed_data.shape}\")\n",
    "print(f\"New columns: {set(processed_data.columns) - set(combined_data.columns)}\")\n",
    "\n",
    "# View a sample of the processed text\n",
    "pd.DataFrame({\n",
    "    'Original': processed_data['review_text'].head(5),\n",
    "    'Processed': processed_data['review_text_cleaned'].head(5)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get preprocessing statistics\n",
    "preprocessing_stats = preprocessor.get_preprocessing_stats(processed_data)\n",
    "print(\"Preprocessing Statistics:\")\n",
    "for key, value in preprocessing_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Save processed data\n",
    "processed_data_path = PROCESSED_DATA_DIR / 'processed_reviews.parquet'\n",
    "processed_data.to_parquet(processed_data_path, index=False)\n",
    "print(f\"Saved processed data to {processed_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "This step extracts and transforms features from the preprocessed data using the `FeatureEngineer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the feature engineer\n",
    "feature_engineer = FeatureEngineer(\n",
    "    text_column='review_text_cleaned',  # Use the cleaned text column\n",
    "    user_id_column='user_id',\n",
    "    product_id_column='product_id',\n",
    "    date_column='date',\n",
    "    rating_column='rating',\n",
    "    tfidf_max_features=3000,\n",
    "    tfidf_ngram_range=(1, 3),\n",
    "    behavioral_time_window=30,\n",
    "    enable_graph_features=True,  # Set to False if performance is slow\n",
    "    enable_sentiment=True,\n",
    "    feature_scaling='standard',\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Split the data into features and target\n",
    "X = processed_data.drop('is_fake', axis=1) if 'is_fake' in processed_data.columns else processed_data\n",
    "y = processed_data['is_fake'] if 'is_fake' in processed_data.columns else None\n",
    "\n",
    "# Fit and transform the feature engineer\n",
    "X_features, feature_names = feature_engineer.fit_transform(X)\n",
    "\n",
    "# Print feature information\n",
    "print(f\"Feature matrix shape: {X_features.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(\"\\nFeature groups:\")\n",
    "print(f\"Text features: {len(feature_engineer.text_feature_names_)}\")\n",
    "print(f\"Behavioral features: {len(feature_engineer.behavioral_feature_names_)}\")\n",
    "print(f\"Graph features: {len(feature_engineer.graph_feature_names_)}\")\n",
    "print(f\"Sentiment features: {len(feature_engineer.sentiment_feature_names_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "\n",
    "# Class distribution\n",
    "if y is not None:\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(f\"Training: {pd.Series(y_train).value_counts(normalize=True).round(3)}\")\n",
    "    print(f\"Testing: {pd.Series(y_test).value_counts(normalize=True).round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "This step trains and optimizes multiple models using the `ModelTrainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model trainer\n",
    "model_trainer = ModelTrainer(\n",
    "    random_state=RANDOM_SEED,\n",
    "    cv_folds=5,\n",
    "    scoring='f1',  # Primary metric for model selection\n",
    "    handle_imbalance=True,\n",
    "    use_bayesian_search=True,  # Set to False for faster but less optimal search\n",
    "    n_iter=20,  # Reduce for faster execution\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train all models\n",
    "model_results = model_trainer.train_all_models(X_train, y_train)\n",
    "\n",
    "# Print results summary\n",
    "print(\"\\nModel Training Results:\")\n",
    "for model_name, results in model_results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(f\"Best score (CV): {results['best_score']:.4f}\")\n",
    "    print(f\"Best parameters: {results['best_params']}\")\n",
    "\n",
    "# Get the best model\n",
    "best_model, best_model_name = model_trainer.get_best_model()\n",
    "print(f\"\\nBest model: {best_model_name} with score {model_trainer.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "This step evaluates the best model on the test set using the `Evaluation` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "evaluator = Evaluation(\n",
    "    model_name=best_model_name,\n",
    "    class_labels=['Genuine', 'Fake'],\n",
    "    pos_label=1,  # 1 is the 'Fake' class\n",
    "    reports_dir=REPORTS_DIR,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Get predicted probabilities if the model supports it\n",
    "try:\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "except (AttributeError, IndexError):\n",
    "    y_pred_proba = None\n",
    "    print(\"Model does not support probability estimates, ROC curve will not be available.\")\n",
    "\n",
    "# Compute evaluation metrics\n",
    "metrics = evaluator.compute_metrics(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm_plot = evaluator.plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Generate ROC curve if probabilities are available\n",
    "if y_pred_proba is not None:\n",
    "    roc_plot = evaluator.plot_roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Generate classification report\n",
    "cr = evaluator.classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Interpretation\n",
    "\n",
    "This step interprets the best model using the `ModelInterpreter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the interpreter\n",
    "interpreter = ModelInterpreter(\n",
    "    model=best_model,\n",
    "    feature_names=feature_names,\n",
    "    class_names=['Genuine', 'Fake'],\n",
    "    save_dir=REPORTS_DIR / 'interpretation',\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Convert sparse matrix to numpy array if needed\n",
    "X_test_array = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n",
    "\n",
    "# Calculate permutation feature importance\n",
    "importance_results = interpreter.permutation_importance(\n",
    "    X_test_array, y_test, n_repeats=5, max_features=20\n",
    ")\n",
    "\n",
    "# Generate SHAP explanations if supported by the model\n",
    "try:\n",
    "    shap_results = interpreter.explain_shap(\n",
    "        X_test_array, \n",
    "        sample_size=min(500, len(X_test_array)),  # Limit to 500 samples for performance\n",
    "        plot_types=['summary', 'bar']\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"SHAP analysis not available for this model: {e}\")\n",
    "\n",
    "# Generate LIME explanations for a few examples\n",
    "try:\n",
    "    # Select a few random samples from test set\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    sample_indices = np.random.choice(len(X_test_array), size=5, replace=False)\n",
    "    \n",
    "    lime_results = interpreter.explain_lime(\n",
    "        X_test_array,  # Training data for LIME explainer\n",
    "        X_test_array[sample_indices],  # Samples to explain\n",
    "        num_features=10,\n",
    "        sample_indices=list(range(len(sample_indices)))\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"LIME analysis not available for this model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Serialization\n",
    "\n",
    "This step saves the best model to disk for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete pipeline including feature engineering\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define a custom transformer that wraps our feature engineer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_engineer):\n",
    "        self.feature_engineer = feature_engineer\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Return just the feature matrix without the feature names\n",
    "        features, _ = self.feature_engineer.transform(X)\n",
    "        return features\n",
    "\n",
    "# Create the complete pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_engineer', FeatureEngineeringTransformer(feature_engineer)),\n",
    "    ('model', best_model)\n",
    "])\n",
    "\n",
    "# Save the pipeline to disk\n",
    "model_path = MODELS_DIR / 'production_model.joblib'\n",
    "joblib.dump(pipeline, model_path)\n",
    "print(f\"Saved production model pipeline to {model_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_parameters': str(best_model.get_params()),\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'metrics': {\n",
    "        metric: float(value) for metric, value in metrics.items()\n",
    "    },\n",
    "    'feature_count': len(feature_names),\n",
    "    'data_points_count': len(X_train) + len(X_test),\n",
    "    'training_set_size': len(X_train),\n",
    "    'test_set_size': len(X_test),\n",
    "    'class_distribution': {\n",
    "        'train': dict(pd.Series(y_train).value_counts()),\n",
    "        'test': dict(pd.Series(y_test).value_counts())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata as JSON\n",
    "import json\n",
    "metadata_path = MODELS_DIR / 'production_model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "print(f\"Saved model metadata to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Testing the Saved Model\n",
    "\n",
    "Let's verify that the saved model works as expected by making a prediction on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = joblib.load(model_path)\n",
    "\n",
    "# Create a small sample of test data\n",
    "sample_reviews = pd.DataFrame({\n",
    "    'review_text': [\n",
    "        \"This product is amazing! Best purchase ever!\",\n",
    "        \"Not worth the money, broke after two days.\",\n",
    "        \"Average product, does what it says but nothing special.\"\n",
    "    ],\n",
    "    'user_id': ['user_1', 'user_2', 'user_3'],\n",
    "    'product_id': ['product_1', 'product_2', 'product_3'],\n",
    "    'rating': [5, 1, 3],\n",
    "    'date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03'])\n",
    "})\n",
    "\n",
    "# Make predictions\n",
    "try:\n",
    "    predictions = loaded_model.predict(sample_reviews)\n",
    "    \n",
    "    # Get probabilities if available\n",
    "    try:\n",
    "        probabilities = loaded_model.predict_proba(sample_reviews)\n",
    "        fake_probs = probabilities[:, 1]  # Probability of being fake\n",
    "    except (AttributeError, IndexError):\n",
    "        fake_probs = None\n",
    "    \n",
    "    # Display results\n",
    "    results = pd.DataFrame({\n",
    "        'Review': sample_reviews['review_text'],\n",
    "        'Rating': sample_reviews['rating'],\n",
    "        'Prediction': ['Fake' if p == 1 else 'Genuine' for p in predictions],\n",
    "    })\n",
    "    \n",
    "    if fake_probs is not None:\n",
    "        results['Fake Probability'] = fake_probs.round(3)\n",
    "    \n",
    "    print(\"Model Predictions on Sample Data:\")\n",
    "    display(results)\n",
    "    \n",
    "    print(\"\\nThe saved model pipeline is working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error testing the model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've implemented a complete end-to-end pipeline for fake review detection:\n",
    "1. Collected and preprocessed review data\n",
    "2. Engineered text, behavioral, graph, and sentiment features\n",
    "3. Trained multiple models and selected the best one based on performance\n",
    "4. Evaluated the model using comprehensive metrics\n",
    "5. Interpreted the model to understand feature importance\n",
    "6. Serialized the complete pipeline for production deployment\n",
    "\n",
    "The final model is now ready for deployment and has been saved to `artifacts/models/production_model.joblib`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
